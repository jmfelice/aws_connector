S3 Module
========

The S3 module provides a comprehensive interface for interacting with Amazon S3, including file uploads, DataFrame handling, and integration with Redshift. It supports both standard and KMS encryption for S3 operations.

Configuration
------------

The module uses the ``S3Config`` class for managing S3 parameters. You can configure it in several ways:

1. Direct initialization:

.. code-block:: python

    from aws_connector import S3Config

    config = S3Config(
        bucket="my-bucket",
        directory="data/",
        iam="123456789012",
        region="us-east-1",
        kms_key_id="arn:aws:kms:region:account:key/key-id",
        max_retries=3,
        timeout=30
    )

2. From environment variables:

.. code-block:: python

    config = S3Config.from_env()

The following environment variables are supported:

- ``AWS_S3_BUCKET``: The S3 bucket name
- ``AWS_S3_DIRECTORY``: The directory path within the bucket
- ``AWS_IAM_ROLE``: The IAM role ARN
- ``AWS_REGION``: The AWS region
- ``AWS_KMS_KEY_ID``: The KMS key ID for encryption
- ``AWS_MAX_RETRIES``: Maximum number of retries for AWS operations
- ``AWS_TIMEOUT``: Timeout in seconds for AWS operations

Basic Usage
----------

1. Uploading a file to S3:

.. code-block:: python

    from aws_connector import S3Connector

    s3 = S3Connector(
        bucket="my-bucket",
        directory="data/"
    )

    # Upload a file
    result = s3.upload_to_s3("path/to/local/file.csv")
    if result['success']:
        print(f"File uploaded successfully: {result['message']}")
    else:
        print(f"Upload failed: {result['error']}")

2. Uploading a DataFrame to S3:

.. code-block:: python

    import pandas as pd

    # Create a sample DataFrame
    df = pd.DataFrame({
        'id': [1, 2, 3],
        'name': ['Alice', 'Bob', 'Charlie']
    })

    # Upload DataFrame
    result = s3.upload_to_s3(
        df,
        table_name="users",
        temp_file_name="users_20240101.csv"
    )

Advanced Usage
-------------

1. Loading data from S3 to Redshift:

.. code-block:: python

    # Create table statement
    create_table = """
    CREATE TABLE IF NOT EXISTS users (
        id INT,
        name VARCHAR(100)
    )
    """

    # Load data to Redshift
    result = s3.upload_to_redshift(
        data=df,
        redshift_schema_name="public",
        redshift_table_name="users",
        redshift_username="admin",
        create_table_statement=create_table,
        echo=True
    )

2. Loading existing S3 file to Redshift:

.. code-block:: python

    result = s3.load_from_s3_to_redshift(
        s3_file_path="data/users_20240101.csv",
        redshift_schema_name="public",
        redshift_table_name="users",
        redshift_username="admin",
        create_table_statement=create_table
    )

3. Error handling:

.. code-block:: python

    from aws_connector.exceptions import UploadError, RedshiftError

    try:
        result = s3.upload_to_s3(df, table_name="users")
    except UploadError as e:
        print(f"Upload error: {e}")
    except RedshiftError as e:
        print(f"Redshift error: {e}")

API Reference
------------

S3Config
~~~~~~~~

.. autoclass:: aws_connector.s3.S3Config
   :members:
   :undoc-members:
   :show-inheritance:

S3Base
~~~~~~

.. autoclass:: aws_connector.s3.S3Base
   :members:
   :undoc-members:
   :show-inheritance:

S3Connector
~~~~~~~~~~

.. autoclass:: aws_connector.s3.S3Connector
   :members:
   :undoc-members:
   :show-inheritance:

Testing
-------

The module is designed to be easily testable. You can override the following methods for testing:

.. code-block:: python

    class MockS3Connector(S3Connector):
        def _get_s3_client(self):
            return MockS3Client()
            
        def _get_redshift_client(self):
            return MockRedshiftClient()

Best Practices
-------------

1. Use appropriate file naming conventions:

.. code-block:: python

    # Use descriptive names with timestamps
    result = s3.upload_to_s3(
        df,
        table_name="users",
        temp_file_name="users_20240101_001.csv"
    )

2. Handle large files with chunked uploads:

.. code-block:: python

    # For large DataFrames, consider chunking
    chunk_size = 10000
    for i in range(0, len(df), chunk_size):
        chunk = df[i:i + chunk_size]
        result = s3.upload_to_s3(
            chunk,
            table_name="users",
            temp_file_name=f"users_chunk_{i}.csv"
        )

3. Use KMS encryption for sensitive data:

.. code-block:: python

    s3 = S3Connector(
        bucket="my-bucket",
        directory="data/",
        kms_key_id="arn:aws:kms:region:account:key/key-id"
    )

4. Monitor upload progress:

.. code-block:: python

    result = s3.upload_to_s3(
        df,
        table_name="users",
        echo=True  # Print progress information
    ) 